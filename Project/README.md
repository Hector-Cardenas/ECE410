# ParallelWaveGAN Hardware Accelerator #

This directory contains work done both through weekly codefests, and in general, on the final project for ECE 410. The work for this project was done in collaboration with Google's Gemini LLM, and the full conversation for works done within can be found [here](https://g.co/gemini/share/235e15774d0e).

## Motivation ##
Though audio generation has made leaps in progress in recent years, recent developments tend to be focused on either software generation for music or hardware implementations that are focused on voice generation; there appears to be a large gap in hardware use of machine learning for musical purposes. As such, I was hoping to look for an audio generation algorithm that could be deployed on an FPGA module for use with Eurorack modular synthesizers. The ultimate goal is to create a self-contained Eurorack module that takes in inputs in the form of control voltage, and uses a machine learning algorithm to output audio samples that can be converted into output voltage.

## Algorithm Choice ##
Research began by looking through modern audio generation algorithms for a viable candidate that would fit our hardware needs; Gemini's "Deep Research" model was particularly helpful in this quest, and the associated conversation can be found [here](https://g.co/gemini/share/6be085720a74). The first algorithm we looked at is Google's Wavenet<sup>1</sup>, one the earliest approaches to ML audio generation. This algorithm served as the inspiration to many of the other algorithms we would eventually look at and features a Dilated Causal Convolutional Network to create the next sample based on a large window of previously generated samples along with a conditional input; as this algorithm was originally intended for voice synthesis, the conditional input is typically composed of text words for the network to convert into audio representations of these words. Dilated, in this context, refers to their technique for increasingly accessing samples that are wider apart as we move up through the network layers; this allows the netork to be influenced by a much larger window of samples and inputs without compromising computational efficiency. This algorithm was deemed unfit for our purposes due to the limitations on real-time performance imposed by its causal (sequential) nature; in order to precict the next sample, the network needs access to the sample immediately preceeding it. 

On the other end of the spectrum are current state-of the art models that rely on transformers in order to turn a description of a song into a full clip representative of the characteristics within that prompt; AudioCraft<sup>2</sup> by Meta is one such example. This is a powerful method, but lacks the direct control that we were seeking for our model; rather than creating entire tracks or songs, the goal was to create waveshapes or granular samples of sound to allow for more direct musical explorations.

The algorithm that we ended up settling on was name ParallelWaveGAN<sup>3</sup>. This algorithm is based off of the original WaveNet structure, but adds a few key improvements that are critical for our use case. Firstly, the parallel nature of this algorithm means that we are creating multiple samples every time-step, allowing us to achieve the real-time performance necessary for deployment as a hardware synthesis module. Secondly, the GAN design allows us to perform the more complicated training computations (discriminator) on a general purpose computer and deploy the simpler output network (generator) on our hardware. Given the timeline of this course project, only the generator network was focused on with the discriminator left for future work.

## Profiling ##
