# Week 1 Challenges # 
For Week 1, we were tasked with exploring the use of Large Language Models as coding collaborators. I've never really used LLMs before as I've previously found that the results they provide were not accurate for the problems that I'd asked them to tackle; even "basic" hardware questions led to many inaccuracies or the LLM talking in circles without providing a solid answer. As such, my main goal was to be open-minded with the LLM and work on improving my own prompting as well.

Though I had initially started with ChatGPT, I immediately ran into the same troubles that I expected from LLMs when I'd heard about "vibe-coding"; coupled with the fact that exporting ChatGPT conversations was not possible at the time of this challenge, I decided to switch to Gemini 2.5 Pro. OpenAI has since added conversation sharing, so my limited conversation with ChatGPT can be found [here](https://chatgpt.com/share/67edbef1-b4ac-8011-bce8-cc13ae062ab3), while the full conversation with Gemini can be found [here](https://g.co/gemini/share/7294a7af8931); the final prompt was added in error while testing Gemini's capabilites with Google Calendar. 

## Challenge 4 ## 
I started this challenge by being vague and fairly blunt with my prompting to ChatGPT; I was essentially asking it to tackle the entire problem and then iterating with short corrections. Partially due to my inexperience, and partially due to wanting to test the capabilities of LLMs as a replacement for coding, I tried asking ChatGPT to analyze its own code output every time it gave me a new codeblock and simply copy/pasted any errors I'd encountered back to it; this was my understanding of "vibe-coding" at the time, and I now know that this was a terrible way to achieve any usable results when collaborating with an LLM. Nonetheless, I decided to cut my losses with ChatGPT and moved instead to using Gemini 2.5 Pro; the latest Google model that had been performing quite well on coding tasts according to benchmarks released at the time.

I ended up liking the code that Gemini was providing me a bit more, as it seemed capable of addressing some of the concerns I'd had with ChatGPTs code before I'd even asked; ```leakage_shift```, for example was set to a reasonable value right away, where ChatGPT had set it to 100 by default. Nonetheless, I was still very inexperienced with prompting and was not able to get the results that I'd wanted. While the LIF model Gemini coded seemed to be what I was asking for, Gemini seemed much less capable of providing a working testbench and repeated attempts to troubleshoot the testbench code just led me to more and more errors and, ultimately, frustration. As such, I was not able to get a succesfully validated LIF Neuron due to being unable to "vibe-code" the testbench.

The results I was hoping for was a little bit different than the paper's, and I was explicitely trying to add a few "improvements" that I thought would make for a better model. Specifically, after my own troubles with getting a succesful testbench, I was dissapointed to notice that the paper failed to include an LLM generated testbench to verify their design; after sitting with this a bit longer, I'm less concerned by it since I believe that the verification should likely be a human task to ensure that the code has been checked and tested by someone who understand the "bigger picture" and can confirm that edge-cases that an LLM may not consider are handled correctly. Additionally, I was attempting to create a slightly less generalized model by adding the ability to go to negative voltage values; I did not consider shooting for something quite as far as a biologically accurate model like the Hodkin-Huxlex model.

# Conclusion #

Ultimately, I learned that "vibe-coding" successfully is a much more involved process that requires clearly defined goals and prompts. I was also fairly impressed with Gemini's improvements over my previous attempts to use LLMs to code, but learned that it still has quite significant limitations when it comes to verification and timing analysis; the ```LIF.sv``` code that it gave me was more than acceptable, but I was never able to get the testbench to work.
